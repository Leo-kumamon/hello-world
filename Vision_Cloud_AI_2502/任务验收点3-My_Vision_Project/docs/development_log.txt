心得体会（大部分基于与ai对话（gpt-5.1），少部分基于视频以及指导书，CSDN博客）

1.配置SSH key--git bash
2.搞清楚了bash是个什么东西
3.利用hello world script,同时利用github pages创建自我介绍网站（从来没有试过的）
https://github.com/Leo-kumamon/hello-world/settings/pages
4.连接ssh时git clone没办法establish，多试一次就搞完了
5.学习了一些基本的网页自我介绍格式，比如title,head,body,h1,h2,p等等格式，意味着字号不一样
6.学会了在git上添加文件，以及默认的时候没有显示add file绿色键的时候直接更改网页为main
就可以添加文件了
7.发布这个网站需要时间，要等几十秒
8.anaconda安装之后，下载pytorch，按照指示安装清华镜像之后安装很慢，改为mamba安装，发现还是不行，后面按照ai的指示，删去default之后，用pip install成功安装
9.个人发现anaconda prompt非常好用，字体默认的比较舒服，按照指示去写命令基本上不会出错，不像py（报错一堆还看不懂）,没有怎么打击我的信心
10.自定义环境torch39，用了py3.9版本，并且因为我的电脑没有NVIDIA独显，所以只能用CPU版
11.发现指导书里面很多东西（包括视频）对于cpu only非常不友好，视频里推荐是用jupyter，但是我还是选择用pycharm
12.在pycharm里面配完基础环境（我自定义的环境torch39/python3.9），以及学会了在pycharm建文件夹，成功用上了pycharm


//我先跳过了CNN曲线，先做了yolov11的
13.下载yolov11：从git里面拿到安装包，在conda下载到torch39的环境里面就可以了（没想到这么简单）
14.通过理解模型的从头到尾的功能：先是yolo团队构建了一个基础的架构，通过一定量做出了一个基础的训练权重，然后我们通过labelimg自行创建属于自己的数据，将这些数据通过py先进行格式转换（xml--txt），再进行模型的训练，然后用需要验证的图片放进去，进而检测里面的object（我用的是people检测人，用car检测车），遇到的问题真的不少
15.学习labelimg，如何操作框图并标记
16.训练脚本我起初以为非常复杂，但是通过ai的指示发现只需要几个必要的部分就可以，训练权重，数据来源，循环次数，图片大小，批量尺寸，设备cpu，worker（不知道是啥，ai教我写0），其实很简单
17.遇到的问题：
	1.ai上下文只有6条记忆，因此后面甚至跟我说了yolov8
	2.图片路径没有设置好（后面才逐渐理解python真的可以在文档层面进行操作）
	3.没有理解image文件夹，annotation文件夹以及labels文件夹的功能，导致将xml格式的放到了image文件夹，导致训练出差错
	4.没有按照ai指示建立文件夹
	5.yaml对应的文件路径没写对
	6.遇到numpy和matplotlib的版本与训练要求的不兼容，重新安装才能解决
	7.成功训练之后，发现推理完的模型跟原来的照片一样：
		1.图片train和val的不是一一对应，导致训练目标出问题	
		2.一一对应后继续推理，发现检测到的框图是0，发现部分文件的txt格式里面并没有东西，导致训练这一部分的时候啥都没有
	8.最终训练出了一个car的模型，但是最后面想训练car和people一起的模型却没有实现
18.最终结果：car和people一起训练的时候后面置信度只有零点零几，后面真的没招了
19。后面误打误撞搞了一个人和车都可以的（非常意外）（我把车的txt文件的内容全删了，然后竟然训练了一个能够识别人和车的模型）

做完这个yolov11，我看了一下我和gpt-5.1的对话条数：86条

//CNN曲线
20.等到做CNN曲线时，时间只剩下一天了，我只能快速请教ai给予我解答
在看了指导书里面关于DNN与CNN的视频，大概了解了一下手写数字识别的原理，后面代码大部分由ai教我写，代码虽然是ai写的，但是ai给我讲解了整一个代码的思路，这样也弥补了一些没有动手实践的不足


